{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5ZGnXarUivz"
      },
      "source": [
        "# Natural Language Processing (NLP)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Si usamos colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "uK9kNw98UkGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGU5DZGSUiv1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYwGbEePUiv3"
      },
      "source": [
        "## Carga de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwB2mPfAUiv5"
      },
      "outputs": [],
      "source": [
        "# Si usamos colab\n",
        "df = pd.read_csv(\"/content/drive/My Drive/nlp/new_drug_train.tsv\", sep = \"\\t\")\n",
        "\n",
        "# Si usamos local\n",
        "# df = pd.read_csv(\"new_drug_train.tsv\", sep = \"\\t\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2z-8e--mUiv5"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eadd_E11Uiv6"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHf7kTIEUiv7"
      },
      "source": [
        "## Preprocesamiento de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-_DypWoUiv8"
      },
      "source": [
        "El preprocesamiento de datos es una tarea importante en el procesamiento de lenguaje natural. En este caso vamos a realizar las siguientes tareas:\n",
        "- Convertir el texto a minúsculas\n",
        "- Eliminar caracteres especiales\n",
        "- Tokenizar el texto\n",
        "- Eliminar las stopwords (palabras comunes que no aportan significado).\n",
        "- Lematizar el texto (reducir las palabras a su raíz)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVnoD7RYUiv8"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "negation_words = {\"no\", \"not\", \"never\", \"none\", \"n't\"}\n",
        "stop_words = stop_words - negation_words\n",
        "stop_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpnbgmzVUiv9"
      },
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1N59SOHAUiv9"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    words = word_tokenize(text)\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "df['clean_review'] = df['review'].apply(preprocess_text)\n",
        "df[['review', 'clean_review']].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4BZHzYnUiv-"
      },
      "outputs": [],
      "source": [
        "df = df.drop(columns = ['review'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJCkZ3pzUiv-"
      },
      "source": [
        "## Split de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spbQo0F7Uiv-"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "random_state = 17\n",
        "\n",
        "X, y = df.loc[:, df.columns != 'rating'], df[\"rating\"]\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    test_size=(1.0/3),\n",
        "    random_state=random_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PvhD4WMUiv-"
      },
      "source": [
        "## Count Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RRpZv9JUiv_"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Ponemos un min_df =0.1 para reducir la cantidad de vectores a solo aquellos que se repiten mas del 2%\n",
        "vectorizer = CountVectorizer(stop_words=\"english\", min_df=0.02)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30fFevcbUiv_"
      },
      "source": [
        "Creamos una matriz con los tokens que aparece cada palabra en cada review. Utilizamos *fit* para que aprenda el vocabulario de los textos (identificar las palabras únicas) y *transform* para convertir convertir cada documentos del corpus en una matriz donde las filas representan las reviews y las columnas las palabras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CuziaJKUiv_"
      },
      "outputs": [],
      "source": [
        "X_train_review_tok_matrix = vectorizer.fit_transform(X_train[\"clean_review\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slnLMQYlUiv_"
      },
      "source": [
        "Para el conjunto de test, solo utilizamos *transform* ya que debemos utilizar el vocabulario aprendido en train. Hacer *fit* implicaría introducir información de test durante el entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzmdA_iFUiwA"
      },
      "outputs": [],
      "source": [
        "X_test_review_tok_matrix = vectorizer.transform(X_test[\"clean_review\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSeKNBn6UiwA"
      },
      "source": [
        "Convertimos las matrices en un DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8illdSfUiwA"
      },
      "outputs": [],
      "source": [
        "X_train_review_tok = pd.DataFrame(X_train_review_tok_matrix.toarray())\n",
        "X_test_review_tok = pd.DataFrame(X_test_review_tok_matrix.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cA4DTOVHUiwA"
      },
      "source": [
        "Concatenamos el conjunto de train con los vectores y hacemos lo mismo en test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sr7AkBGvUiwA"
      },
      "outputs": [],
      "source": [
        "X_train.reset_index(drop=True, inplace=True)\n",
        "X_test.reset_index(drop=True, inplace=True)\n",
        "X_train_review_tok.reset_index(drop=True, inplace=True)\n",
        "X_test_review_tok.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jc00tGS0UiwB"
      },
      "outputs": [],
      "source": [
        "X_train_concat = pd.concat([X_train, X_train_review_tok], axis=1)\n",
        "X_test_concat = pd.concat([X_test, X_test_review_tok], axis=1)\n",
        "X_train_concat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlGaLb3LUiwB"
      },
      "source": [
        "### Modelo de clasificación"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usaremos un árbol de decisión para predecir el rating."
      ],
      "metadata": {
        "id": "8VkZ-ogoUr6E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INHXUVfnUiwB"
      },
      "outputs": [],
      "source": [
        "X_train_concat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QlJoANiUiwB"
      },
      "source": [
        "#### Predecimos usando solamente los datos de las reviews."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQtQnj2MUiwB"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "tree = DecisionTreeClassifier(max_depth=10, criterion='gini', random_state=42)\n",
        "\n",
        "tree.fit(X_train_concat.iloc[:, 6:], y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HtYJta12UiwB"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "y_train_pred = tree.predict(X_train_concat.iloc[:, 6:])\n",
        "y_test_pred = tree.predict(X_test_concat.iloc[:, 6:])\n",
        "\n",
        "print(\"Train accuracy: \", accuracy_score(y_train, y_train_pred))\n",
        "print(\"Test accuracy: \", accuracy_score(y_test, y_test_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaauDyHNUiwC"
      },
      "source": [
        "## TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ot7vs-5gUiwC"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tf_idf_vectorizer = TfidfVectorizer(stop_words=\"english\", min_df=0.02)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKFlyDFfUiwC"
      },
      "outputs": [],
      "source": [
        "tfidf_X_train_review_tok_matrix = tf_idf_vectorizer.fit_transform(X_train[\"clean_review\"])\n",
        "tfidf_X_test_review_tok_matrix = tf_idf_vectorizer.transform(X_test[\"clean_review\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlYEz1lNUiwC"
      },
      "outputs": [],
      "source": [
        "tfidf_X_train_review_tok = pd.DataFrame(tfidf_X_train_review_tok_matrix.toarray())\n",
        "tfidf_X_test_review_tok = pd.DataFrame(tfidf_X_test_review_tok_matrix.toarray())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLroODOpUiwC"
      },
      "outputs": [],
      "source": [
        "tfidf_X_train_review_tok.reset_index(drop=True, inplace=True)\n",
        "tfidf_X_test_review_tok.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqOqGdnZUiwD"
      },
      "outputs": [],
      "source": [
        "tfidf_X_train_concat = pd.concat([X_train, tfidf_X_train_review_tok], axis=1)\n",
        "tfidf_X_test_concat = pd.concat([X_test, tfidf_X_test_review_tok], axis=1)\n",
        "tfidf_X_test_concat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1Rg38nzUiwD"
      },
      "source": [
        "## Modelo de clasificación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEI_uK1tUiwD"
      },
      "outputs": [],
      "source": [
        "tree = DecisionTreeClassifier(max_depth=30, criterion='gini', random_state=42)\n",
        "\n",
        "tree.fit(tfidf_X_train_concat.iloc[:, 6:], y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ibtCHatUiwD"
      },
      "outputs": [],
      "source": [
        "y_train_pred = tree.predict(tfidf_X_train_concat.iloc[:, 6:])\n",
        "y_test_pred = tree.predict(tfidf_X_test_concat.iloc[:, 6:])\n",
        "\n",
        "print(\"Train accuracy: \", accuracy_score(y_train, y_train_pred))\n",
        "print(\"Test accuracy: \", accuracy_score(y_test, y_test_pred))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}