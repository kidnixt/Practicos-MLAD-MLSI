{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5ZGnXarUivz"
      },
      "source": [
        "# üìö Natural Language Processing (NLP)\n",
        "\n",
        "En esta pr√°ctica vamos a trabajar con herramientas de procesamiento de lenguaje natural utilizando la biblioteca `nltk`. Exploraremos t√©cnicas fundamentales como la tokenizaci√≥n, eliminaci√≥n de palabras vac√≠as, lematizaci√≥n y modelado de texto.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Si usamos colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "uK9kNw98UkGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üß∞ Imports\n",
        "\n",
        "\n",
        "- `punkt`: para dividir texto en palabras y oraciones.\n",
        "- `stopwords`: lista de palabras comunes que suelen eliminarse.\n",
        "- `wordnet`: base de datos l√©xica utilizada para la lematizaci√≥n."
      ],
      "metadata": {
        "id": "fkSjawboW-Al"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGU5DZGSUiv1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYwGbEePUiv3"
      },
      "source": [
        "# üì• Carga de datos\n",
        "\n",
        "üìå El dataset contiene rese√±as de medicamentos y su respectiva calificaci√≥n (`rating`), √∫til para tareas de clasificaci√≥n de sentimientos o an√°lisis de texto.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwB2mPfAUiv5"
      },
      "outputs": [],
      "source": [
        "# Si usamos colab\n",
        "df = pd.read_csv(\"/content/drive/My Drive/data/new_drug_train.tsv\", sep = \"\\t\")\n",
        "\n",
        "# Si usamos local\n",
        "# df = pd.read_csv(\"new_drug_train.tsv\", sep = \"\\t\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2z-8e--mUiv5"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eadd_E11Uiv6"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-_DypWoUiv8"
      },
      "source": [
        "# üßπ Preprocesamiento de Texto\n",
        "\n",
        "Antes de alimentar un modelo con texto, es fundamental limpiarlo y normalizarlo. Las tareas realizadas incluyen:\n",
        "\n",
        "- üî° Conversi√≥n a min√∫sculas.\n",
        "- ‚ùå Eliminaci√≥n de caracteres no alfab√©ticos (como puntuaci√≥n).\n",
        "- ‚úÇÔ∏è Tokenizaci√≥n (dividir texto en palabras).\n",
        "- üßΩ Eliminaci√≥n de stopwords (palabras comunes que no aportan mucho significado).\n",
        "- üå± Lematizaci√≥n (reducir palabras a su forma base).\n",
        "\n",
        "Tambi√©n se remueve la columna `review` original, dejando solo el texto limpio.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVnoD7RYUiv8"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "negation_words = {\"no\", \"not\", \"never\", \"none\", \"n't\"}\n",
        "stop_words = stop_words - negation_words\n",
        "print(len(stop_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpnbgmzVUiv9"
      },
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1N59SOHAUiv9"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    words = word_tokenize(text)\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "df['clean_review'] = df['review'].apply(preprocess_text)\n",
        "df[['review', 'clean_review']].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4BZHzYnUiv-"
      },
      "outputs": [],
      "source": [
        "df = df.drop(columns = ['review'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJCkZ3pzUiv-"
      },
      "source": [
        "# ‚úÇÔ∏è Divisi√≥n de Datos\n",
        "\n",
        "Separamos el conjunto de datos en entrenamiento y test utilizando `train_test_split`.\n",
        "\n",
        "Esto nos permite entrenar el modelo con una parte del dataset y evaluar su desempe√±o con otra que no ha visto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spbQo0F7Uiv-"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "random_state = 17\n",
        "\n",
        "X, y = df.loc[:, df.columns != 'rating'], df[\"rating\"]\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    test_size=(1.0/3),\n",
        "    random_state=random_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PvhD4WMUiv-"
      },
      "source": [
        "# üî¢ Count Vectorizer\n",
        "\n",
        "Transformamos el texto en una representaci√≥n num√©rica mediante el m√©todo `Bag of Words`, usando `CountVectorizer`.\n",
        "\n",
        "- Cada columna representa una palabra del vocabulario.\n",
        "- Cada fila representa una review.\n",
        "- El valor indica cu√°ntas veces aparece una palabra en la review.\n",
        "\n",
        "> üîç Usamos `min_df=0.02` para filtrar palabras poco frecuentes y reducir la dimensionalidad."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RRpZv9JUiv_"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Ponemos un min_df = 0.2 para reducir la cantidad de vectores a solo aquellos que se repiten mas del 2%\n",
        "vectorizer = CountVectorizer(stop_words=\"english\", min_df=0.02)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìê Fit vs Transform\n",
        "\n",
        "Aplicamos `.fit_transform()` sobre el conjunto de entrenamiento para construir el vocabulario y vectorizar.\n",
        "\n",
        "Para el conjunto de test, usamos solamente `.transform()` para evitar \"mirar\" datos del test durante el entrenamiento (data leakage).\n"
      ],
      "metadata": {
        "id": "dCb2ZBwvaUkE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30fFevcbUiv_"
      },
      "source": [
        "Creamos una matriz con los tokens que aparece cada palabra en cada review. Utilizamos *fit* para que aprenda el vocabulario de los textos (identificar las palabras √∫nicas) y *transform* para convertir convertir cada documentos del corpus en una matriz donde las filas representan las reviews y las columnas las palabras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CuziaJKUiv_"
      },
      "outputs": [],
      "source": [
        "X_train_review_tok_matrix = vectorizer.fit_transform(X_train[\"clean_review\"])\n",
        "X_test_review_tok_matrix = vectorizer.transform(X_test[\"clean_review\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSeKNBn6UiwA"
      },
      "source": [
        "## üìä De matriz dispersa a DataFrame\n",
        "\n",
        "Convertimos la matriz esparsa resultante en un `DataFrame` de pandas para facilitar su uso junto con las otras variables predictoras.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8illdSfUiwA"
      },
      "outputs": [],
      "source": [
        "X_train_review_tok = pd.DataFrame(X_train_review_tok_matrix.toarray())\n",
        "X_test_review_tok = pd.DataFrame(X_test_review_tok_matrix.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cA4DTOVHUiwA"
      },
      "source": [
        "## üîó Unificaci√≥n del dataset\n",
        "\n",
        "Combinamos los vectores num√©ricos generados a partir del texto con las otras variables del conjunto de datos original.\n",
        "\n",
        "Esto permite entrenar modelos que consideren tanto texto como otras posibles variables estructuradas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sr7AkBGvUiwA"
      },
      "outputs": [],
      "source": [
        "X_train.reset_index(drop=True, inplace=True)\n",
        "X_test.reset_index(drop=True, inplace=True)\n",
        "X_train_review_tok.reset_index(drop=True, inplace=True)\n",
        "X_test_review_tok.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jc00tGS0UiwB"
      },
      "outputs": [],
      "source": [
        "X_train_concat = pd.concat([X_train, X_train_review_tok], axis=1)\n",
        "X_test_concat = pd.concat([X_test, X_test_review_tok], axis=1)\n",
        "X_train_concat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlGaLb3LUiwB"
      },
      "source": [
        "## üå≥ Modelo de Clasificaci√≥n\n",
        "\n",
        "Entrenaremos un modelo de √Årbol de Decisi√≥n (`DecisionTreeClassifier`) para predecir la calificaci√≥n del medicamento (`rating`), usando los datos vectorizados de las reviews.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQtQnj2MUiwB"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "tree = DecisionTreeClassifier(max_depth=10, criterion='gini', random_state=42)\n",
        "\n",
        "tree.fit(X_train_concat.iloc[:, 6:], y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HtYJta12UiwB"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Realizar predicciones en los conjuntos de entrenamiento y prueba\n",
        "y_train_pred = tree.predict(X_train_concat.iloc[:, 6:])\n",
        "y_test_pred = tree.predict(X_test_concat.iloc[:, 6:])\n",
        "\n",
        "# Calcular y guardar las precisiones\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "# Imprimir las precisiones para verificaci√≥n\n",
        "print(\"Train accuracy: \", train_accuracy)\n",
        "print(\"Test accuracy: \", test_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaauDyHNUiwC"
      },
      "source": [
        "# üìò TF-IDF Vectorizer\n",
        "\n",
        "A diferencia del `CountVectorizer`, que solo cuenta apariciones de palabras, el `TfidfVectorizer` asigna pesos seg√∫n la importancia de cada palabra en el documento y en el corpus.\n",
        "\n",
        "- **TF** (*Term Frequency*): Frecuencia de una palabra en el documento.\n",
        "- **IDF** (*Inverse Document Frequency*): Penaliza palabras comunes en todos los documentos.\n",
        "\n",
        "> ‚öñÔ∏è Esto ayuda a reducir el peso de palabras que aparecen en casi todos los textos y resaltar las m√°s informativas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ot7vs-5gUiwC"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tf_idf_vectorizer = TfidfVectorizer(stop_words=\"english\", min_df=0.02)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKFlyDFfUiwC"
      },
      "outputs": [],
      "source": [
        "tfidf_X_train_review_tok_matrix = tf_idf_vectorizer.fit_transform(X_train[\"clean_review\"])\n",
        "tfidf_X_test_review_tok_matrix = tf_idf_vectorizer.transform(X_test[\"clean_review\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlYEz1lNUiwC"
      },
      "outputs": [],
      "source": [
        "tfidf_X_train_review_tok = pd.DataFrame(tfidf_X_train_review_tok_matrix.toarray())\n",
        "tfidf_X_test_review_tok = pd.DataFrame(tfidf_X_test_review_tok_matrix.toarray())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLroODOpUiwC"
      },
      "outputs": [],
      "source": [
        "tfidf_X_train_review_tok.reset_index(drop=True, inplace=True)\n",
        "tfidf_X_test_review_tok.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqOqGdnZUiwD"
      },
      "outputs": [],
      "source": [
        "tfidf_X_train_concat = pd.concat([X_train, tfidf_X_train_review_tok], axis=1)\n",
        "tfidf_X_test_concat = pd.concat([X_test, tfidf_X_test_review_tok], axis=1)\n",
        "tfidf_X_test_concat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1Rg38nzUiwD"
      },
      "source": [
        "## üå≥ Modelo de Clasificaci√≥n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEI_uK1tUiwD"
      },
      "outputs": [],
      "source": [
        "tree = DecisionTreeClassifier(max_depth=10, criterion='gini', random_state=42)\n",
        "\n",
        "tree.fit(tfidf_X_train_concat.iloc[:, 6:], y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ibtCHatUiwD"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "y_train_pred_tfidf = tree.predict(tfidf_X_train_concat.iloc[:, 6:])\n",
        "y_test_pred_tfidf = tree.predict(tfidf_X_test_concat.iloc[:, 6:])\n",
        "\n",
        "train_accuracy_tfidf = accuracy_score(y_train, y_train_pred_tfidf)\n",
        "test_accuracy_tfidf = accuracy_score(y_test, y_test_pred_tfidf)\n",
        "\n",
        "print(\"Train accuracy (TF-IDF): \", train_accuracy_tfidf)\n",
        "print(\"Test accuracy (TF-IDF): \", test_accuracy_tfidf)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparaci√≥n M√©tricas"
      ],
      "metadata": {
        "id": "tmySYRFyeWKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = {\n",
        "    'M√©trica': ['Precisi√≥n en Entrenamiento', 'Precisi√≥n en Prueba'],\n",
        "    'Modelo con CountVectorizer': [train_accuracy, test_accuracy],\n",
        "    'Modelo con TF-IDF': [train_accuracy_tfidf, test_accuracy_tfidf]\n",
        "}\n",
        "\n",
        "df_comparacion_modelos = pd.DataFrame(data)\n",
        "\n",
        "df_comparacion_modelos"
      ],
      "metadata": {
        "id": "mA65SBzreVnj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "_5ZGnXarUivz",
        "fkSjawboW-Al",
        "ZYwGbEePUiv3",
        "b-_DypWoUiv8",
        "PJCkZ3pzUiv-",
        "2PvhD4WMUiv-",
        "CaauDyHNUiwC",
        "tmySYRFyeWKa"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}