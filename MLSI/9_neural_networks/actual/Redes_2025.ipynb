{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQNRtRAqRgm5"
      },
      "source": [
        "# Redes Neuronales"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmXXJsMugF-M"
      },
      "source": [
        "## Introducci√≥n\n",
        "\n",
        "Las redes neuronales artificiales son modelos que aprenden funciones complejas a partir de datos. En este pr√°ctico, trabajaremos con un problema de **regresi√≥n**: predecir el precio de viviendas a partir de caracter√≠sticas como n√∫mero de habitaciones, edad, etc.\n",
        "\n",
        "Exploraremos distintas arquitecturas de redes neuronales y analizaremos c√≥mo afecta su complejidad al desempe√±o del modelo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43FlqJf6SnAQ"
      },
      "source": [
        "## Dependencias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlnS9rZkRHZt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TngldqhSCo3"
      },
      "source": [
        "## Carga de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGjeObR6RHZ5"
      },
      "outputs": [],
      "source": [
        "# Cargar el dataset de ejemplo que est√° en Keras\n",
        "from tensorflow.keras.datasets import boston_housing\n",
        "\n",
        "(X_train, y_train), (X_val, y_val) = boston_housing.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrvfoV75SKRd"
      },
      "source": [
        "### Shapes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7FpYrRARHZ7"
      },
      "outputs": [],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvYgHE5ZRHZ-"
      },
      "outputs": [],
      "source": [
        "y_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1lKMGgCRHaC"
      },
      "source": [
        "## Arquitectura de la Red Neuronal\n",
        "El modelo que estamos utilizando es una red neuronal **secuencial** de tipo **feedforward**, dise√±ada para resolver un problema de **regresi√≥n** (predecir el precio de una casa).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XvD5l9MRHaI"
      },
      "outputs": [],
      "source": [
        "model_1 = keras.Sequential([\n",
        "    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),  # Capa oculta con la entrada siendo la cantidad de features\n",
        "    layers.Dense(32, activation='relu'),  # Capa oculta\n",
        "    layers.Dense(1)  # Capa de salida (el precio a predecir)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XgPuZryg17I"
      },
      "source": [
        "- `Dense(64, activation='relu')`: primera capa oculta, con 64 neuronas. Utiliza la funci√≥n ReLU para introducir no linealidad. Esta capa recibe como entrada un vector de 13 caracter√≠sticas (una por atributo del dataset).\n",
        "  \n",
        "- `Dense(32, activation='relu')`: segunda capa oculta, con menor cantidad de neuronas. Se busca una representaci√≥n m√°s compacta de los datos.\n",
        "\n",
        "- `Dense(1)`: capa de salida. Produce un √∫nico valor continuo correspondiente al precio predicho."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLhiZkyaRHaK"
      },
      "outputs": [],
      "source": [
        "model_1.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2U3UH2LFg7IV"
      },
      "source": [
        "### üîç ¬øPor qu√© esta arquitectura?\n",
        "- Las redes densas son adecuadas cuando no hay estructura espacial en los datos (como ocurre con im√°genes).\n",
        "- La funci√≥n `ReLU` es r√°pida de computar y evita problemas de desvanecimiento del gradiente.\n",
        "- La capa de salida tiene una sola neurona porque estamos prediciendo un √∫nico valor num√©rico (no una clase)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHFLmQa5hHUK"
      },
      "source": [
        "## üîß Compilaci√≥n del Modelo\n",
        "\n",
        "La compilaci√≥n del modelo especifica **c√≥mo ser√° entrenado**. En este caso, usamos:\n",
        "\n",
        "- **Optimizador: `adam`**  \n",
        "  Un optimizador eficiente que ajusta autom√°ticamente la tasa de aprendizaje de cada peso.\n",
        "\n",
        "- **Funci√≥n de p√©rdida: `mean_squared_error` (MSE)**  \n",
        "  Es la funci√≥n que el modelo intenta minimizar. Penaliza m√°s los errores grandes, lo que puede ayudar a evitar grandes desviaciones en las predicciones.\n",
        "\n",
        "- **M√©trica: `mae` (Mean Absolute Error)**  \n",
        "  M√©trica adicional que se reporta durante el entrenamiento. Es √∫til porque est√° en las mismas unidades que el objetivo (precio de las casas), y es f√°cil de interpretar.\n",
        "\n",
        "> üí° El modelo entrenar√° para minimizar MSE, pero durante el proceso tambi√©n se mostrar√° el MAE como referencia de desempe√±o.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-55ACgeNRHaO"
      },
      "outputs": [],
      "source": [
        "model_1.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6rXMYCShXxk"
      },
      "source": [
        "## üöÄ Entrenamiento del Modelo\n",
        "\n",
        "Utilizamos el m√©todo `fit()` para entrenar la red neuronal. Este m√©todo ajusta los pesos del modelo para minimizar la funci√≥n de p√©rdida.\n",
        "\n",
        "### Par√°metros clave:\n",
        "\n",
        "- **`X_train`, `y_train`**: conjunto de entrenamiento (entradas y salidas).\n",
        "- **`validation_data=(X_val, y_val)`**: conjunto de validaci√≥n que permite evaluar el modelo en datos no vistos, detectando sobreajuste.\n",
        "- **`epochs=100`**: n√∫mero de veces que el modelo ver√° todo el conjunto de entrenamiento.\n",
        "- **`batch_size=32`**: tama√±o de los bloques de datos usados para actualizar los pesos.\n",
        "\n",
        "Al final, la variable `history` almacena el historial del entrenamiento, incluyendo la p√©rdida y las m√©tricas por √©poca. Esto nos permite visualizar c√≥mo evoluciona el modelo con el tiempo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-I2JxZnTWvY"
      },
      "outputs": [],
      "source": [
        "history_1 = model_1.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=100, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4W0dvFB0RHaU"
      },
      "source": [
        "## Evaluacion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QavNWRgYcgk"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_training_history(history):\n",
        "    train_loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    train_mae = history.history['mae']\n",
        "    val_mae = history.history['val_mae']\n",
        "\n",
        "    epochs = range(1, len(train_loss) + 1)\n",
        "\n",
        "    plt.figure(figsize=(8, 4))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, train_loss, 'b-', label='P√©rdida en entrenamiento')\n",
        "    plt.plot(epochs, val_loss, 'r-', label='P√©rdida en validaci√≥n')\n",
        "    plt.xlabel('√âpocas')\n",
        "    plt.ylabel('P√©rdida (Loss)')\n",
        "    plt.title('Curva de p√©rdida en entrenamiento y validaci√≥n')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, train_mae, 'orange', label='MAE en entrenamiento')\n",
        "    plt.plot(epochs, val_mae, 'green', label='MAE en validaci√≥n')\n",
        "    plt.xlabel('√âpocas')\n",
        "    plt.ylabel('MAE (Error Absoluto Medio)')\n",
        "    plt.title('Curva de MAE en entrenamiento y validaci√≥n')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMVasZ0FVVvA"
      },
      "outputs": [],
      "source": [
        "plot_training_history(history_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HW5yDk_Ph5i2"
      },
      "source": [
        "- Si las curvas de entrenamiento y validaci√≥n bajan y se estabilizan juntas ‚Üí el modelo aprende correctamente.\n",
        "- Si la p√©rdida de validaci√≥n empieza a subir mientras la de entrenamiento sigue bajando ‚Üí el modelo est√° **sobreajustando** (*overfitting*).\n",
        "- Si ambas se mantienen altas ‚Üí el modelo **no est√° aprendiendo bien** (*underfitting*)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQ1Vp33TRHaV"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3WYadN4UToA"
      },
      "outputs": [],
      "source": [
        "y_pred_1 = model_1.predict(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LidWveoiRHaX"
      },
      "outputs": [],
      "source": [
        "mae_1 = mean_absolute_error(y_val, y_pred_1)\n",
        "mse_1 = mean_squared_error(y_val, y_pred_1)\n",
        "print(f\"MAE: {mae_1}\")\n",
        "print(f\"MSE: {mse_1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ykh2qGeOT85t"
      },
      "source": [
        "## Segundo modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vU7RIMkVFXc"
      },
      "outputs": [],
      "source": [
        "model_2 = keras.Sequential([\n",
        "    layers.Dense(500, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    layers.Dense(200, activation='relu'),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(1)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPFIpOD4iPKC"
      },
      "outputs": [],
      "source": [
        "model_2.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDC0qWrZiRbN"
      },
      "source": [
        "- `Dense(500, activation='relu')`: capa muy ancha que permite capturar combinaciones complejas entre las entradas.\n",
        "- Capas subsiguientes: tama√±os decrecientes (200, 128, 64, 32), siguiendo una forma de \"embudo\".\n",
        "- Cada capa aplica una funci√≥n `ReLU`, que introduce no linealidad y permite aprender funciones m√°s complejas.\n",
        "- La salida (`Dense(1)`) es un √∫nico valor continuo, ideal para regresi√≥n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBYsidNVVWZO"
      },
      "outputs": [],
      "source": [
        "model_2.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bu9PldZrVWw7"
      },
      "outputs": [],
      "source": [
        "history_2 = model_2.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=500, batch_size=32, verbose=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uG0IZvNeigJO"
      },
      "source": [
        "\n",
        "- **`epochs=500`**: permite que la red tenga m√°s tiempo para ajustar sus pesos y aprender patrones complejos. Sin embargo, debemos tener cuidado con el **sobreajuste (overfitting)**.\n",
        "- **`verbose=False`**: desactiva la salida en consola para no saturar el notebook con 500 l√≠neas. Para evaluar el progreso, es importante **graficar la evoluci√≥n de la p√©rdida y del MAE** usando el historial (`history`).\n",
        "\n",
        "> üí° El aumento de √©pocas debe ir acompa√±ado de una buena estrategia de evaluaci√≥n para saber cu√°ndo el modelo deja de mejorar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSYmqp75VdpI"
      },
      "outputs": [],
      "source": [
        "plot_training_history(history_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SBydfUlj4qd"
      },
      "outputs": [],
      "source": [
        "y_pred_2 = model_2.predict(X_val)\n",
        "mae_2 = mean_absolute_error(y_val, y_pred_2)\n",
        "mse_2 = mean_squared_error(y_val, y_pred_2)\n",
        "print(f\"MAE: {mae_2}\")\n",
        "print(f\"MSE: {mse_2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvRpbeBwYxlf"
      },
      "source": [
        "## Tercer modelo (con regularizaci√≥n)\n",
        "\n",
        "Este modelo mantiene la misma arquitectura profunda del segundo modelo, pero **agrega una capa de regularizaci√≥n** para mejorar su capacidad de generalizaci√≥n.\n",
        "\n",
        "### üîß ¬øQu√© cambia?\n",
        "\n",
        "- Se a√±ade `Dropout(0.5)` antes de la capa de salida. Esta capa \"apaga\" aleatoriamente el 50% de las neuronas durante el entrenamiento.\n",
        "- Esto fuerza al modelo a **no depender de caminos espec√≠ficos** y lo hace m√°s robusto frente a nuevos datos.\n",
        "\n",
        "### üéØ ¬øPor qu√© usar Dropout?\n",
        "\n",
        "- Ayuda a prevenir el **sobreajuste (overfitting)**, especialmente en redes con muchas capas y neuronas.\n",
        "- Mejora la capacidad del modelo de **generalizar**.\n",
        "- Es una de las formas m√°s comunes y efectivas de **regularizaci√≥n en redes neuronales**.\n",
        "\n",
        "> üí° Dropout solo est√° activo durante el entrenamiento. En predicci√≥n, se desactiva autom√°ticamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKhdUV0AY7Nc"
      },
      "outputs": [],
      "source": [
        "model_3 = keras.Sequential([\n",
        "    layers.Dense(500, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    layers.Dense(200, activation='relu'),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(1)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nmNI6ONZWi1"
      },
      "outputs": [],
      "source": [
        "model_3.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3D2FfApjJV3"
      },
      "source": [
        "\n",
        "### Early Stopping\n",
        "\n",
        "Entrenar una red neuronal por muchas √©pocas puede llevar al **sobreajuste**, donde el modelo aprende demasiado bien los datos de entrenamiento pero pierde capacidad de generalizaci√≥n.\n",
        "\n",
        "Para evitarlo, utilizamos `EarlyStopping`, una t√©cnica que:\n",
        "\n",
        "- **Monitorea el desempe√±o del modelo en el conjunto de validaci√≥n** (`val_loss`).\n",
        "- **Detiene autom√°ticamente el entrenamiento** si no hay mejora tras un n√∫mero definido de √©pocas (en este caso, 30).\n",
        "- **Restaura los pesos √≥ptimos** alcanzados durante el entrenamiento (`restore_best_weights=True`).\n",
        "\n",
        "### üîß Ventajas:\n",
        "- Ahorra tiempo.\n",
        "- Evita el sobreajuste.\n",
        "- Mantiene el mejor modelo encontrado autom√°ticamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6GolD13ZW9D"
      },
      "outputs": [],
      "source": [
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=30,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "history_3 = model_3.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=500, batch_size=32, callbacks=[early_stopping], verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JREFcQGaZy7X"
      },
      "outputs": [],
      "source": [
        "plot_training_history(history_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tib6TDQQkGzU"
      },
      "outputs": [],
      "source": [
        "y_pred_3 = model_3.predict(X_val)\n",
        "mae_3 = mean_absolute_error(y_val, y_pred_3)\n",
        "mse_3 = mean_squared_error(y_val, y_pred_3)\n",
        "print(f\"MAE: {mae_3}\")\n",
        "print(f\"MSE: {mse_3}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2ajh0yNkLMi"
      },
      "source": [
        "## Resumen M√©tricas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dA9IDzFvkPQI"
      },
      "outputs": [],
      "source": [
        "#create dataframe of the mae and mse\n",
        "df_metrics = pd.DataFrame({'Modelo': ['Modelo 1', 'Modelo 2', 'Modelo 3'], 'MAE': [mae_1, mae_2, mae_3], 'MSE': [mse_1, mse_2, mse_3]})\n",
        "df_metrics\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
