{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b81ff9a9",
   "metadata": {},
   "source": [
    "# Práctico: Ensembles en Machine Learning\n",
    "\n",
    "En este práctico exploraremos diferentes técnicas de ensamblado de modelos (ensembles) usando `scikit-learn`. Los ensembles permiten combinar varios modelos base para mejorar el rendimiento y la robustez de las predicciones.\n",
    "\n",
    "## Objetivos\n",
    "\n",
    "- Comprender el funcionamiento de los métodos de ensemble más populares: Bagging, Random Forest, AdaBoost y Gradient Boosting.\n",
    "- Comparar el desempeño de un árbol de decisión individual con modelos ensemble.\n",
    "- Analizar la importancia de las variables y la estabilidad de los modelos.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dc2608",
   "metadata": {},
   "source": [
    "## 1. Carga de datos\n",
    "\n",
    "Utilizaremos el dataset de vinos (`wine`) incluido en `scikit-learn`. Este dataset es útil para clasificación multiclase y tiene variables numéricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a98efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "import pandas as pd\n",
    "\n",
    "data = load_wine()\n",
    "X = data.data\n",
    "y = data.target\n",
    "df = pd.DataFrame(X, columns=data.feature_names)\n",
    "df['target'] = y\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b0a827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimir la descripción del dataset\n",
    "print(data.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13a48c4",
   "metadata": {},
   "source": [
    "## 2. Árbol de Decisión Individual\n",
    "\n",
    "Entrenaremos un árbol de decisión y evaluaremos su desempeño como modelo base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f407544f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "tree.fit(X_train, y_train)\n",
    "y_pred_tree = tree.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0de1544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización del árbol entrenado\n",
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plot_tree(tree, feature_names=data.feature_names, class_names=data.target_names, filled=True)\n",
    "plt.title(\"Árbol de Decisión Individual (max_depth=3)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8fb4cf",
   "metadata": {},
   "source": [
    "## 3. Bagging\n",
    "\n",
    "El Bagging (Bootstrap Aggregating) entrena varios modelos independientes sobre subconjuntos aleatorios de los datos y promedia sus predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370818d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "bagging = BaggingClassifier(DecisionTreeClassifier(), n_estimators=100, random_state=42)\n",
    "bagging.fit(X_train, y_train)\n",
    "y_pred_bag = bagging.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred_bag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73e3d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de uno de los árboles del ensemble de Bagging\n",
    "plt.figure(figsize=(12, 8))\n",
    "plot_tree(bagging.estimators_[0], feature_names=data.feature_names, class_names=data.target_names, filled=True)\n",
    "plt.title(\"Ejemplo de Árbol en Bagging\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc4b0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curva de error en train y test variando n_estimators en Bagging\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "n_range = range(1, 101, 5)\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for n in n_range:\n",
    "    bag = BaggingClassifier(DecisionTreeClassifier(), n_estimators=n, random_state=42)\n",
    "    bag.fit(X_train, y_train)\n",
    "    train_scores.append(1 - accuracy_score(y_train, bag.predict(X_train)))\n",
    "    test_scores.append(1 - accuracy_score(y_test, bag.predict(X_test)))\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(n_range, train_scores, label=\"Error Train\")\n",
    "plt.plot(n_range, test_scores, label=\"Error Test\")\n",
    "plt.xlabel(\"n_estimators\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.title(\"Error vs n_estimators en Bagging\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae8d664",
   "metadata": {},
   "source": [
    "**Ejercicio:** Implementa un BaggingClassifier usando regresión logística como estimador base (`LogisticRegression`). Compara su desempeño con el Bagging de árboles y discute en qué casos podría ser preferible cada uno."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a7a4d2",
   "metadata": {},
   "source": [
    "## 4. Random Forest\n",
    "\n",
    "Random Forest es una extensión de Bagging donde cada árbol ve solo un subconjunto aleatorio de variables en cada división."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c8dfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d5156e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de uno de los árboles del Random Forest\n",
    "plt.figure(figsize=(12, 8))\n",
    "plot_tree(rf.estimators_[0], feature_names=data.feature_names, class_names=data.target_names, filled=True)\n",
    "plt.title(\"Ejemplo de Árbol en Random Forest\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b1e32e",
   "metadata": {},
   "source": [
    "**Ejemplo:** Visualiza la importancia de las variables en el Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9f0930",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rf.feature_importances_\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(data.feature_names, importances)\n",
    "plt.xlabel(\"Importancia\")\n",
    "plt.title(\"Importancia de las variables según Random Forest\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d833f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curva de error en train y test variando n_estimators en Random Forest\n",
    "train_scores_rf = []\n",
    "test_scores_rf = []\n",
    "\n",
    "for n in n_range:\n",
    "    rf_tmp = RandomForestClassifier(n_estimators=n, random_state=42)\n",
    "    rf_tmp.fit(X_train, y_train)\n",
    "    train_scores_rf.append(1 - accuracy_score(y_train, rf_tmp.predict(X_train)))\n",
    "    test_scores_rf.append(1 - accuracy_score(y_test, rf_tmp.predict(X_test)))\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(n_range, train_scores_rf, label=\"Error Train\")\n",
    "plt.plot(n_range, test_scores_rf, label=\"Error Test\")\n",
    "plt.xlabel(\"n_estimators\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.title(\"Error vs n_estimators en Random Forest\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b940ea9f",
   "metadata": {},
   "source": [
    "## 5. AdaBoost\n",
    "\n",
    "AdaBoost ajusta secuencialmente modelos, enfocándose en los ejemplos mal clasificados por los modelos anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da55992c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "ada = AdaBoostClassifier(estimator=LogisticRegression(max_iter=10000), n_estimators=50, learning_rate=1.0, random_state=42)\n",
    "ada.fit(X_train, y_train)\n",
    "y_pred_ada = ada.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred_ada))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83e52c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curva de error en train y test variando n_estimators en AdaBoost\n",
    "train_scores_ada = []\n",
    "test_scores_ada = []\n",
    "\n",
    "for n in n_range:\n",
    "    ada_tmp = AdaBoostClassifier(n_estimators=n, learning_rate=1.0, random_state=42, algorithm=\"SAMME\")\n",
    "    ada_tmp.fit(X_train, y_train)\n",
    "    train_scores_ada.append(1 - accuracy_score(y_train, ada_tmp.predict(X_train)))\n",
    "    test_scores_ada.append(1 - accuracy_score(y_test, ada_tmp.predict(X_test)))\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(n_range, train_scores_ada, label=\"Error Train\")\n",
    "plt.plot(n_range, test_scores_ada, label=\"Error Test\")\n",
    "plt.xlabel(\"n_estimators\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.title(\"Error vs n_estimators en AdaBoost\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8aa3f6f",
   "metadata": {},
   "source": [
    "**Ejercicio:** Implementa un modelo de Boosting utilizando regresión logística como estimador base (`LogisticRegression`). Compara su desempeño con el Boosting basado en árboles y discute en qué situaciones podría ser útil esta alternativa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1998bd",
   "metadata": {},
   "source": [
    "## 6. Gradient Boosting\n",
    "\n",
    "Gradient Boosting construye modelos secuenciales, cada uno corrigiendo los errores del anterior usando gradientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923bc249",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "gb.fit(X_train, y_train)\n",
    "y_pred_gb = gb.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred_gb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f108d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curva de error en train y test variando n_estimators en Gradient Boosting\n",
    "train_scores_gb = []\n",
    "test_scores_gb = []\n",
    "\n",
    "for n in n_range:\n",
    "    gb_tmp = GradientBoostingClassifier(n_estimators=n, learning_rate=0.1, random_state=42)\n",
    "    gb_tmp.fit(X_train, y_train)\n",
    "    train_scores_gb.append(1 - accuracy_score(y_train, gb_tmp.predict(X_train)))\n",
    "    test_scores_gb.append(1 - accuracy_score(y_test, gb_tmp.predict(X_test)))\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(n_range, train_scores_gb, label=\"Error Train\")\n",
    "plt.plot(n_range, test_scores_gb, label=\"Error Test\")\n",
    "plt.xlabel(\"n_estimators\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.title(\"Error vs n_estimators en Gradient Boosting\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e798ff13",
   "metadata": {},
   "source": [
    "## 7. Comparación de Resultados\n",
    "\n",
    "Resume y compara los resultados obtenidos con cada técnica. ¿Cuál fue el mejor modelo? ¿Por qué?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1946759e",
   "metadata": {},
   "source": [
    "## 8. Preguntas para reflexionar\n",
    "\n",
    "- ¿Por qué los ensembles suelen superar a los modelos individuales?\n",
    "- ¿Qué riesgos tiene usar ensembles muy complejos?\n",
    "- ¿Cómo elegir entre Bagging y Boosting?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
